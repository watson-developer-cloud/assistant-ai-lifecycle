<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Speech and text - Watson Assistant web chat toolkit</title>
  <!-- This is the external library provided by Watson Speed that provides the TTS and STT functionality. You can
  find more details about this library here: https://github.com/watson-developer-cloud/speech-javascript-sdk. -->
  <script src="watson-speech.min.js"></script>
  <link rel="stylesheet" href="../../../../carbon/carbon-components.min.css">

  <style>
    #WACContainer.WACContainer .RecordButton.bx--btn {
      width: 100%;
      max-width: unset;
      padding: 0;
      justify-content: center;
      margin-bottom: 1px;
    }
  </style>
</head>
<body>

  <script>
    let ttsAuthToken;
    let sttAuthToken;
    let webChatInstance;
    let currentStream;
    let isRecording = false;

    /**
     * This function will take the given message response received from Watson Assistant and generate the text from it
     * that we wish to be converted to speech.
     */
    function generateTextFromMessage(message) {
      // This code here only supports text responses but it can be updated to support additional types of messages such
      // as "option" responses (buttons and dropdowns) or "connect_to_agent" response. You can find more information
      // about the possible types of responses here: https://cloud.ibm.com/apidocs/assistant/assistant-v2#message-response.
      return message.output.generic.map(message => message.text).join(' ');
    }

    /**
     * This code is the handler that is called when web chat receives a message response from Watson Assistant. It will
     * convert that response into text and then use Watson Speech to convert the text into speech and play it from the
     * browser.
     */
    function handleMessageReceive(event) {
      const synthText = generateTextFromMessage(event.data);
      const audio = WatsonSpeech.TextToSpeech.synthesize({
        text: synthText,
        accessToken: ttsAuthToken,
      });
      audio.onerror = function (error) {
        console.error('TextToSpeech error', error);
      }
    }

    /**
     * After we have converted speech to text, this function is called to send that text as a message to the assistant.
     */
    function sendTextToAssistant(text) {
      const sendObject = { input: { text } };
      webChatInstance.send(sendObject)
    }

    /**
     * This function is called when we wish to start recording speech.
     */
    function onStartRecord() {
      const stream = WatsonSpeech.SpeechToText.recognizeMicrophone({
        accessToken: sttAuthToken,
        model: 'en-US_BroadbandModel',
        objectMode: true,
      });

      stream.on('data', function (data) {
        console.log('Received SpeechToText data', data)
        if (data.results[0]?.final) {
          stream.stop();
          currentStream = null;
          console.log('Stopped listening');
          setButtonState(false);

          const text = data.results[0].alternatives.map(message => message.transcript).join(' ');
          sendTextToAssistant(text);
        }
      });

      stream.on('error', function (error) {
        console.error('SpeechToText error', error);
      });

      currentStream = stream;
    }

    /**
     * This function is called when we wish to stop recording speech.
     */
    function onStopRecord() {
      currentStream?.stop();
      currentStream = null;
    }

    /**
     * This updates the state of the button to reflect whether or not it is currently recording.
     */
    function setButtonState(localIsRecording) {
      isRecording = localIsRecording;

      const label = isRecording ? 'Stop' : 'Start';
      document.querySelector('.RecordButton').innerHTML = `${label} recording`;
    }

    /**
     * This is the function that is called when the user clicks on the record button.
     */
    function onButtonClick() {
      setButtonState(!isRecording);
      if (isRecording) {
        onStartRecord();
      } else {
        onStopRecord();
      }
    }

    /**
     * This function is called to add the record button to the web chat UI.
     */
    function addRecordButton(instance) {
      const button = document.createElement('button');
      button.classList.add('RecordButton');
      button.classList.add('bx--btn');
      button.classList.add('bx--btn--primary');
      button.innerHTML = 'Start recording';
      button.addEventListener('click', onButtonClick);

      // You can put this button wherever you like including on the host page that is behind web chat. Web chat also
      // provides a number of places where you can add custom content. These places are called "writable elements". You
      // are free to put any content into these elements you want. The code below uses the writable element that is
      // located just above the input field. You can find more information about the available writable elements here:
      // https://web-chat.global.assistant.watson.cloud.ibm.com/docs.html?to=api-instance-methods#writeableelements
      instance.writeableElements.beforeInputElement.appendChild(button);
    }

    /**
     * This is the function that is called when the web chat code has been loaded and it is ready to be rendered.
     */
    async function onLoad(instance) {
      // Record this instance so we can use it to send messages later.
      webChatInstance = instance;

      // Before you can use the TTS and STT services, this code will need to request an authorization token that it can
      // use. This token will be generated by server code that has access to your secret API keys. In this example we
      // are using a local Express server for generating these tokens. See the getAuthToken.js file in the server folder
      // for this example for more information. In production you would access the token using your own server or by
      // using something like a cloud function hosted in the IBM cloud.
      //
      // Make sure you have started the server before running this example.
      const result = await fetch('http://localhost:3001/getAuthToken');
      const { ttsToken, sttToken } = await result.json();
      ttsAuthToken = ttsToken;
      sttAuthToken = sttToken;

      // Register the handler that will process messages received from the assistant so they can be converted to speech.
      instance.on({ type: 'receive', handler: handleMessageReceive })

      // Add a record button to the UI that can be used to start and stop recording speech.
      addRecordButton(instance);

      // This is not normally required but we want to bypass the home screen for this example.
      instance.updateHomeScreenConfig({ is_on: false });

      instance.render();
    }

    // This is the standard web chat configuration object. You can modify these values with the embed code for your
    // own assistant if you wish to try this example with your assistant. You can find the documentation for this at
    // https://web-chat.global.assistant.watson.cloud.ibm.com/docs.html?to=api-configuration#configurationobject.
    window.watsonAssistantChatOptions = {
      integrationID: "07b05ae0-7e2e-47d1-a309-d0f5b9915ac5",
      region: "us-south",
      serviceInstanceID: "9a3613d2-3ce6-4928-8eb6-4d659d87ae68",
      onLoad: onLoad,
    };
    setTimeout(function(){const t=document.createElement('script');t.src="https://web-chat.global.assistant.watson.appdomain.cloud/versions/" + (window.watsonAssistantChatOptions.clientVersion || 'latest') + "/WatsonAssistantChatEntry.js";document.head.appendChild(t);});
  </script>

</body>
</html>